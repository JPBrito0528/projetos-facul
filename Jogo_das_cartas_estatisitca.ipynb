{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF3wEWmreJuK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from numba import njit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jogo das cartas"
      ],
      "metadata": {
        "id": "QPuwcB263fc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "face_cards = ['A', 'K', 'Q', 'J'] * 4\n",
        "number_cards = [str(i) for i in range(2, 11)] * 4\n",
        "\n",
        "#take out 3 face cards at random\n",
        "chosen_face_cards = random.sample(face_cards, 3)\n",
        "for card in chosen_face_cards:\n",
        "    face_cards.remove(card)\n",
        "\n",
        "\n",
        "#take out 1 non-face cards at random\n",
        "chosen_number_cards = random.sample(number_cards, 1)\n",
        "for card in chosen_number_cards:\n",
        "    number_cards.remove(card)\n",
        "\n",
        "#make one unshuffled deck\n",
        "deck = face_cards + number_cards\n",
        "\n",
        "#shuffle the deck\n",
        "random.shuffle(deck)\n",
        "\n",
        "#seperate male from female\n",
        "male = deck[0:24]\n",
        "female = deck[24:49]\n",
        "\n",
        "#count prompted cards\n",
        "pm = 0\n",
        "for i in range(len(male)):\n",
        "  if male[i] in [str(i) for i in range(2, 11)]:\n",
        "    pm += 1\n",
        "\n",
        "pf = 0\n",
        "for i in range(len(female)):\n",
        "  if female[i] in [str(i) for i in range(2, 11)]:\n",
        "    pf += 1\n",
        "\n",
        "#proportion of prompted\n",
        "porportion = pm/24 - pf/24\n",
        "\n",
        "#result\n",
        "print(porportion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlIZxJ-neLUd",
        "outputId": "4ff988b2-fbab-4127-850a-9373124c8d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.20833333333333337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_porportion(num_yes,num_no,group_size_1,group_size_2):\n",
        "\n",
        "  YES = np.array(['YES']*num_yes)\n",
        "  NO = np.array(['NO']*num_no)\n",
        "\n",
        "  TOTAL = np.concatenate((YES,NO))\n",
        "  np.random.shuffle(TOTAL)\n",
        "\n",
        "  group1 = TOTAL[:group_size_1]\n",
        "  group2 = TOTAL[(group_size_1 + 1):]  # Corrected indexing to include elements up to group_size_2\n",
        "\n",
        "  #count prompted cards\n",
        "  pm = 0\n",
        "  for i in range(group_size_1):\n",
        "    if group1[i] == 'YES':\n",
        "      pm += 1\n",
        "\n",
        "  pf = 0\n",
        "  for i in range(group_size_2):\n",
        "    if group2[i] == 'NO':\n",
        "      pf += 1\n",
        "\n",
        "  #proportion of prompted\n",
        "  porportion = pm/group_size_1 - pf/group_size_2\n",
        "\n",
        "  #result\n",
        "  print(porportion)"
      ],
      "metadata": {
        "id": "6M6ra8qPlxSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercicio dos votos"
      ],
      "metadata": {
        "id": "hoIYT_AM3h7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pop_size = 250000000\n",
        "\n",
        "possible_entries = np.array(['support'] * int(0.88 * pop_size) + ['not'] * int(0.12 * pop_size))\n",
        "\n",
        "# 2. Sample 1000 entries without replacement.\n",
        "np.random.seed(42)  # Optional: Set a seed for reproducibility\n",
        "sampled_entries = np.random.choice(possible_entries, size=1000, replace=False)\n",
        "\n",
        "# 3. Compute p-hat: count the number that are \"support\",\n",
        "# then divide by the sample size.\n",
        "p_hat = np.sum(sampled_entries == 'support') / 1000\n",
        "\n",
        "# Print the result\n",
        "print(\"p-hat:\", p_hat)"
      ],
      "metadata": {
        "id": "vnxAkTYj4OMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2e3be7-b03c-4f69-fbaa-de55a72b6ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p-hat: 0.881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determinar Z-score e p_value"
      ],
      "metadata": {
        "id": "KXnUehv8zWVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "# Step 4: Calculate the Z-score for a 95% confidence level\n",
        "confidence_level = 0.95\n",
        "z_score = norm.ppf((1 + confidence_level) / 2)  # 97.5th percentile for a two-tailed 95% CI\n",
        "\n",
        "z_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQDLcM_ozYYQ",
        "outputId": "f0f90cd6-4caf-4d45-e82b-f6d4fcd8ea11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.959963984540054"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Given data\n",
        "X = 85          # The value for which we want to calculate the z-score\n",
        "mu = 100        # Population mean\n",
        "sigma = 15      # Population standard deviation\n",
        "\n",
        "# Step 1: Calculate the z-score\n",
        "z = (X - mu) / sigma\n",
        "print(f\"Calculated Z-Score: {z}\")\n",
        "\n",
        "# Step 2: Find the CDF value for the z-score\n",
        "cdf_value = norm.cdf(z)\n",
        "print(f\"CDF Value for Z-Score {z}: {cdf_value}\")\n"
      ],
      "metadata": {
        "id": "ivWMTbG_k5X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we can determine how unusual a p_value is with the formula\n",
        "p_value = 0.41\n",
        "p_0 = 0.5\n",
        "n = 850\n",
        "SE = (p_0 * (1 - p_0) / n) ** 0.5\n",
        "Z = (p_value - p_0) /SE #The sample proportion is 5.26 standard errors away from the hy-\n",
        "#2pothesized value. Is this considered unusually low"
      ],
      "metadata": {
        "id": "P9cQ-YOI2JT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given Z-score\n",
        "Z = -5.26\n",
        "\n",
        "# Calculate the p-value for a two-tailed test\n",
        "# This version correctly finds the two-tailed p-value\n",
        "p_value = 2 * (1 - norm.cdf(abs(Z)))\n",
        "\n",
        "p_value"
      ],
      "metadata": {
        "id": "62FlAmEh6E3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e857f674-4d94-4ccc-e1ca-61283a3a5af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.440554016074458e-07"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#confidence interval = p^ +- Z_score * SE\n",
        "\n",
        "#TLC:Independence: The sample is random, and 670 < 10% of all\n",
        "#Americans, therefore we can assume that one respondent’s\n",
        "#response is independent of another.\n",
        "\n",
        "#Success-failure: 571 people answered correctly (successes)\n",
        "#and 99 answered incorrectly (failures), both are greater than 10.\n",
        "\n",
        "#CI: point estimate ± margin of error\n",
        "#HT: Use Z= (point estimate -null value) /SE to find appropriate p-value.\n",
        "\n",
        "#When dealing with counts and investigating how far the observed\n",
        "#counts are from the expected counts, we use a new test statistic\n",
        "#called the chi-square (/shi ^ 2) statistic."
      ],
      "metadata": {
        "id": "8VV5XwwU6mQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SE_proportions = np.sqrt((p1(1-p1))/n1 + (p2(1-p2))/n2)"
      ],
      "metadata": {
        "id": "SMtspquQ6_JN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "33697803-2259-47d4-d385-acebd57cbefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'p1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7e018157518f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSE_proportions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'p1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shi = np.sum(((O-E)**2)/E)"
      ],
      "metadata": {
        "id": "bz7aRTxR_vWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exame ano passado"
      ],
      "metadata": {
        "id": "g9M2S5etxcfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "obGMtFrJGgcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)"
      ],
      "metadata": {
        "id": "I0g8Kf0UGhja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#H0: miu_NAmes = miu_OldTown\n",
        "#H1: miu_NAmes > miu_OldTown\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Load the dataset\n",
        "# Assume 'PROB1.csv' has columns 'Neighborhood' and 'SalePrice'\n",
        "data = pd.read_csv('PROB1.csv')\n",
        "\n",
        "# Filter data for NAmes and OldTown\n",
        "n_ames_prices = data[data['Neighborhood'] == 'NAmes']['SalePrice']\n",
        "old_town_prices = data[data['Neighborhood'] == 'OldTown']['SalePrice']\n",
        "\n",
        "# Perform two-sample t-test (one-tailed since we are testing if NAmes > OldTown)\n",
        "t_stat, p_value = stats.ttest_ind(n_ames_prices, old_town_prices, alternative='greater')\n",
        "\n",
        "# Significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Output the result\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < alpha:x\n",
        "    print(\"Reject the null hypothesis. The average price of houses in NAmes is significantly higher than in OldTown.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. The data does not provide sufficient evidence that NAmes houses are more expensive than OldTown.\")\n"
      ],
      "metadata": {
        "id": "pAghnkqTxfK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('PROB1.csv')\n",
        "\n",
        "# Filter data for NAmes and OldTown\n",
        "n_ames_prices = data[data['Neighborhood'] == 'NAmes']['SalePrice']\n",
        "old_town_prices = data[data['Neighborhood'] == 'OldTown']['SalePrice']\n",
        "\n",
        "# Calculate sample means\n",
        "mean_n_ames = np.mean(n_ames_prices)\n",
        "mean_old_town = np.mean(old_town_prices)\n",
        "\n",
        "# Calculate sample standard deviations\n",
        "std_n_ames = np.std(n_ames_prices, ddof=1)  # Sample standard deviation\n",
        "std_old_town = np.std(old_town_prices, ddof=1)\n",
        "\n",
        "# Calculate sample sizes\n",
        "n_n_ames = len(n_ames_prices)\n",
        "n_old_town = len(old_town_prices)\n",
        "\n",
        "# Calculate the t-statistic manually\n",
        "t_stat = (mean_n_ames - mean_old_town) / sqrt((std_n_ames**2 / n_n_ames) + (std_old_town**2 / n_old_town))\n",
        "\n",
        "# Calculate degrees of freedom (df)\n",
        "df = n_n_ames + n_old_town - 2\n",
        "\n",
        "# You can print the t-statistic, but you will need to look up the p-value in a table or calculate it externally\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"Degrees of Freedom: {df}\")\n",
        "\n",
        "# Significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# If t-statistic is greater than critical value from the t-table, reject the null hypothesis.\n",
        "# You can look up the p-value in a t-distribution table, or use an online calculator."
      ],
      "metadata": {
        "id": "YbiTTyENGdjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b)"
      ],
      "metadata": {
        "id": "2CGx-faWGlvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from math import sqrt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('AMESHOUSING.CSV')\n",
        "\n",
        "# Filter data for NAmes and OldTown\n",
        "n_ames_prices = data[data['Neighborhood'] == 'NAmes']['SalePrice']\n",
        "old_town_prices = data[data['Neighborhood'] == 'OldTown']['SalePrice']\n",
        "\n",
        "# Generate at least 5 pairs of random samples (same sizes as initial samples)\n",
        "sample_size = 100  # Example size, adjust according to actual sample sizes\n",
        "n_samples = 5\n",
        "\n",
        "n_ames_samples = [n_ames_prices.sample(sample_size, random_state=i).reset_index(drop=True) for i in range(n_samples)]\n",
        "old_town_samples = [old_town_prices.sample(sample_size, random_state=i).reset_index(drop=True) for i in range(n_samples)]\n",
        "\n",
        "# Calculate means and standard deviations for each sample\n",
        "means = []\n",
        "std_devs = []\n",
        "\n",
        "for i in range(n_samples):\n",
        "    mean_n_ames = n_ames_samples[i].mean()\n",
        "    mean_old_town = old_town_samples[i].mean()\n",
        "    std_n_ames = n_ames_samples[i].std(ddof=1)\n",
        "    std_old_town = old_town_samples[i].std(ddof=1)\n",
        "\n",
        "    # Calculate the standard error\n",
        "    se = sqrt((std_n_ames ** 2 / sample_size) + (std_old_town ** 2 / sample_size))\n",
        "\n",
        "    # Calculate the t-value for 95% CI\n",
        "    df = 2 * sample_size - 2  # Pooled degrees of freedom\n",
        "    t_critical = stats.t.ppf(1 - 0.025, df)  # 95% CI\n",
        "\n",
        "    # Calculate Confidence Interval\n",
        "    ci_lower = (mean_n_ames - mean_old_town) - t_critical * se\n",
        "    ci_upper = (mean_n_ames - mean_old_town) + t_critical * se\n",
        "\n",
        "    means.append((mean_n_ames, mean_old_town))\n",
        "    std_devs.append((std_n_ames, std_old_town))\n",
        "\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Mean NAmes: {mean_n_ames:.2f}, Mean OldTown: {mean_old_town:.2f}\")\n",
        "    print(f\"  95% Confidence Interval for the difference: ({ci_lower:.2f}, {ci_upper:.2f})\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R_r6zEOLzTn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c)"
      ],
      "metadata": {
        "id": "mOKnEfruKPSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_n_ames = 100  # Sample size for NAmes\n",
        "n_old_town = 100  # Sample size for OldTown\n",
        "\n",
        "# Generate synthetic sale prices (assumed normal distribution)\n",
        "mean_n_ames = 200000  # Mean price for NAmes\n",
        "std_n_ames = 25000     # Standard deviation for NAmes\n",
        "n_ames_prices = np.random.normal(mean_n_ames, std_n_ames, n_n_ames)\n",
        "\n",
        "mean_old_town = 180000  # Mean price for OldTown\n",
        "std_old_town = 30000     # Standard deviation for OldTown\n",
        "old_town_prices = np.random.normal(mean_old_town, std_old_town, n_old_town)\n",
        "\n",
        "# Create a DataFrame to mimic the original data structure\n",
        "data = pd.DataFrame({\n",
        "    'Neighborhood': ['NAmes'] * n_n_ames + ['OldTown'] * n_old_town,\n",
        "    'SalePrice': np.concatenate((n_ames_prices, old_town_prices))\n",
        "})\n",
        "\n",
        "# Step 2: Filter data for NAmes and OldTown\n",
        "n_ames_prices = data[data['Neighborhood'] == 'NAmes']['SalePrice']\n",
        "old_town_prices = data[data['Neighborhood'] == 'OldTown']['SalePrice']\n",
        "\n",
        "# Calculate sample statistics\n",
        "mean_n_ames = np.mean(n_ames_prices)\n",
        "mean_old_town = np.mean(old_town_prices)\n",
        "std_n_ames = np.std(n_ames_prices, ddof=1)  # Sample standard deviation\n",
        "std_old_town = np.std(old_town_prices, ddof=1)  # Sample standard deviation\n",
        "n_n_ames = len(n_ames_prices)\n",
        "n_old_town = len(old_town_prices)\n",
        "\n",
        "# Step 3: Calculate the difference in means\n",
        "mean_diff = mean_n_ames - mean_old_town\n",
        "\n",
        "# Calculate the standard error of the difference\n",
        "se_diff = np.sqrt((std_n_ames**2 / n_n_ames) + (std_old_town**2 / n_old_town))\n",
        "\n",
        "# Get the critical t-value for 95% confidence interval (two-tailed, df= min(n1, n2) - 1)\n",
        "alpha = 0.05\n",
        "df = min(n_n_ames, n_old_town) - 1  # degrees of freedom\n",
        "t_crit = stats.t.ppf(1 - alpha/2, df)\n",
        "\n",
        "# Compute the margin of error\n",
        "margin_of_error = t_crit * se_diff\n",
        "\n",
        "# Compute the confidence interval\n",
        "ci_lower = mean_diff - margin_of_error\n",
        "ci_upper = mean_diff + margin_of_error\n",
        "\n",
        "# Output the results\n",
        "print(f\"Mean difference: {mean_diff:.2f}\")\n",
        "print(f\"95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n"
      ],
      "metadata": {
        "id": "TLSOPhfVKN-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d)"
      ],
      "metadata": {
        "id": "XoR3IHTEKUUg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppM2sxNCKVTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2"
      ],
      "metadata": {
        "id": "t30HR06cKYZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)"
      ],
      "metadata": {
        "id": "ABduCoggKZWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Step 1: Load Data\n",
        "# Assume data.csv is a file with columns: year_built, remodeled, sale_price\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Explore the Data\n",
        "print(data.head())\n",
        "print(data.describe())\n",
        "\n",
        "# Create binary variable for remodeling\n",
        "data['remodeled'] = data['remodeled'].apply(lambda x: 1 if x else 0)\n",
        "\n",
        "# Step 3: Visualize Data\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='year_built', y='sale_price', hue='remodeled', data=data)\n",
        "plt.title('Sale Price by Year Built and Remodeling Status')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Statistical Analysis\n",
        "# Let's use Ordinary Least Squares (OLS) regression to analyze the relationship\n",
        "X = data[['year_built', 'remodeled']]\n",
        "y = data['sale_price']\n",
        "\n",
        "# Add a constant term for the intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Print the summary of the regression\n",
        "print(model.summary())\n",
        "\n",
        "# Step 5: Draw Inferences\n",
        "# Here you can interpret the coefficients and p-values from the model summary\n",
        "\n",
        "# Step 6: Report Findings\n",
        "# Save the findings as a DataFrame and export to a CSV file\n",
        "findings = model.summary2().tables[1]\n",
        "findings.to_csv('findings.csv')"
      ],
      "metadata": {
        "id": "cXZFSlOrKZ_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}